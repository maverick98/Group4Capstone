{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "y3PVm73kLwfK",
        "KujElPN-Orji"
      ],
      "authorship_tag": "ABX9TyPyZejPT8MoEpevdK9Jrt81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maverick98/Group4Capstone/blob/main/decoder_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "y3PVm73kLwfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras.layers import Embedding,Dense, Layer,TextVectorization\n",
        "from tensorflow.keras.backend import softmax\n",
        "from pickle import load, dump, HIGHEST_PROTOCOL\n",
        "from sklearn.utils import shuffle\n",
        "from numpy import savetxt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import convert_to_tensor, int64\n",
        "from tensorflow.keras.layers import LayerNormalization,   ReLU, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow import data, train,  reduce_sum,  equal, argmax,GradientTape, function\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from time import time\n",
        "from pickle import dump\n",
        "from tensorflow import    linalg, ones, maximum, newaxis\n",
        "from tensorflow.keras import Model\n",
        "from pickle import load\n",
        "from tensorflow import Module\n",
        "from tensorflow import   TensorArray, argmax,  transpose\n",
        "from matplotlib.pylab import plt\n",
        "from numpy import arange\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from glob import glob\n",
        "import PIL\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "#import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from PIL import Image\n",
        "from numpy import random\n",
        "from importlib.machinery import SourceFileLoader\n",
        "from os.path import join\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "n6BDkBbELu8q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if connected to GPU"
      ],
      "metadata": {
        "id": "KujElPN-Orji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_5u_9YyOpNz",
        "outputId": "c03fa524-9bcb-4c5f-d88d-5255c50bd08f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "XM1nAEvvKm1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixOYqR2r7T-Q",
        "outputId": "07e04afb-09d4-4025-a7bc-238e040df879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT,force_remount=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJ = \"MyDrive/Capstone/src\" \n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "print(\"PROJECT_PATH from your Google Drive is \",PROJECT_PATH)\n",
        "!rm -rf \"{PROJECT_PATH}\"\n",
        "!mkdir  \"{PROJECT_PATH}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCYv8FkDM6B_",
        "outputId": "f7d33a7e-5b75-488d-8d87-fa568e09a71c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROJECT_PATH from your Google Drive is  /content/drive/MyDrive/Capstone/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone the code from https://sourceforge.net/projects/group4capstone/"
      ],
      "metadata": {
        "id": "IVa7Vdx4d2NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"{PROJECT_PATH}\"\n",
        "!pwd\n",
        "MY_USER_NAME = 'msahu98' # This is your sourceforge.net username\n",
        "!git clone https://{MY_USER_NAME}@git.code.sf.net/p/group4capstone/code group4capstone-code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBHIG28dd6Dp",
        "outputId": "f116f5e4-f079-4e65-be0e-7dd189f7d2af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Capstone/src\n",
            "/content/drive/MyDrive/Capstone/src\n",
            "Cloning into 'group4capstone-code'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 50 (delta 25), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (50/50), 6.35 KiB | 8.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mGLrsMzd1SO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Modules"
      ],
      "metadata": {
        "id": "X2WOLpG1LoqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_PATH \n",
        "#/content/drive/MyDrive/Capstone/src/group4capstone-code/src/positional_encoding.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oQgm8lMlfVuq",
        "outputId": "27afbe6a-50c1-4862-bc43-8f00e3c52528"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Capstone/src'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_module(module_name):\n",
        "    module_py=module_name+'.py'\n",
        "    SourceFileLoader(module_name, join(join(PROJECT_PATH,'group4capstone-code/src'), module_py)).load_module()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZjQo8Ho19CS1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_module('positional_encoding')\n",
        "load_module('multihead_attention')\n",
        "load_module('add_normalization')\n",
        "load_module('feedforward')\n",
        "load_module('decoder')"
      ],
      "metadata": {
        "id": "5WQfX_GiMKN_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Decoder"
      ],
      "metadata": {
        "id": "VftGApgSmT2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from decoder import Decoder"
      ],
      "metadata": {
        "id": "IzHONqJkm-2h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec_vocab_size=20\n",
        "input_sequence_length=5\n",
        "h=8\n",
        "d_k=64\n",
        "d_v=64\n",
        "d_ff=2048\n",
        "d_model=512\n",
        "num_layers=6\n",
        "\n",
        "batch_size=64\n",
        "dropout_rate=0.1\n",
        "\n",
        "input_seq=random.random((batch_size,input_sequence_length))\n",
        "enc_output=random.random((batch_size,input_sequence_length,d_model))\n",
        "\n",
        "decoder=Decoder(dec_vocab_size,input_sequence_length,h,d_k,d_v,d_model,d_ff,num_layers,dropout_rate)\n",
        "\n",
        "print(decoder(input_seq, enc_output,None,True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi3pluGAmaVZ",
        "outputId": "6a838e84-f2b9-451c-8fd5-1d7e1853c536"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[ 0.13178475 -1.7642318   1.0571268  ...  0.8291508  -1.1990172\n",
            "   -0.50617254]\n",
            "  [ 0.18242565 -1.8032032   1.1028193  ...  0.84894186 -1.1993027\n",
            "   -0.4923751 ]\n",
            "  [ 0.19774483 -1.8794407   1.1253523  ...  0.8565719  -1.1833655\n",
            "   -0.47125438]\n",
            "  [ 0.1580991  -1.9222977   1.1086171  ...  0.83932555 -1.160672\n",
            "   -0.44662338]\n",
            "  [ 0.09598697 -1.8996563   1.079998   ...  0.7948668  -1.1515318\n",
            "   -0.4362285 ]]\n",
            "\n",
            " [[ 0.13178475 -1.7642318   1.0571268  ...  0.8291508  -1.1990172\n",
            "   -0.50617254]\n",
            "  [ 0.18242565 -1.8032032   1.1028193  ...  0.84894186 -1.1993027\n",
            "   -0.4923751 ]\n",
            "  [ 0.19774483 -1.8794407   1.1253523  ...  0.8565719  -1.1833655\n",
            "   -0.47125438]\n",
            "  [ 0.1580991  -1.9222977   1.1086171  ...  0.83932555 -1.160672\n",
            "   -0.44662338]\n",
            "  [ 0.09598697 -1.8996563   1.079998   ...  0.7948668  -1.1515318\n",
            "   -0.4362285 ]]\n",
            "\n",
            " [[ 0.13178475 -1.7642318   1.0571268  ...  0.8291508  -1.1990172\n",
            "   -0.50617254]\n",
            "  [ 0.18242565 -1.8032032   1.1028193  ...  0.84894186 -1.1993027\n",
            "   -0.4923751 ]\n",
            "  [ 0.19774483 -1.8794407   1.1253523  ...  0.8565719  -1.1833655\n",
            "   -0.47125438]\n",
            "  [ 0.1580991  -1.9222977   1.1086171  ...  0.83932555 -1.160672\n",
            "   -0.44662338]\n",
            "  [ 0.09598697 -1.8996563   1.079998   ...  0.7948668  -1.1515318\n",
            "   -0.4362285 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.13178475 -1.7642318   1.0571268  ...  0.8291508  -1.1990172\n",
            "   -0.50617254]\n",
            "  [ 0.18242565 -1.8032032   1.1028193  ...  0.84894186 -1.1993027\n",
            "   -0.4923751 ]\n",
            "  [ 0.19774483 -1.8794407   1.1253523  ...  0.8565719  -1.1833655\n",
            "   -0.47125438]\n",
            "  [ 0.1580991  -1.9222977   1.1086171  ...  0.83932555 -1.160672\n",
            "   -0.44662338]\n",
            "  [ 0.09598697 -1.8996563   1.079998   ...  0.7948668  -1.1515318\n",
            "   -0.4362285 ]]\n",
            "\n",
            " [[ 0.13178475 -1.7642318   1.0571268  ...  0.8291508  -1.1990172\n",
            "   -0.50617254]\n",
            "  [ 0.18242565 -1.8032032   1.1028193  ...  0.84894186 -1.1993027\n",
            "   -0.4923751 ]\n",
            "  [ 0.19774483 -1.8794407   1.1253523  ...  0.8565719  -1.1833655\n",
            "   -0.47125438]\n",
            "  [ 0.1580991  -1.9222977   1.1086171  ...  0.83932555 -1.160672\n",
            "   -0.44662338]\n",
            "  [ 0.09598697 -1.8996563   1.079998   ...  0.7948668  -1.1515318\n",
            "   -0.4362285 ]]\n",
            "\n",
            " [[ 0.13178422 -1.7642313   1.0571274  ...  0.8291514  -1.1990172\n",
            "   -0.5061721 ]\n",
            "  [ 0.1824253  -1.8032035   1.10282    ...  0.84894145 -1.1993022\n",
            "   -0.49237594]\n",
            "  [ 0.19774401 -1.8794402   1.1253529  ...  0.85657185 -1.1833651\n",
            "   -0.4712547 ]\n",
            "  [ 0.15809877 -1.9222975   1.1086173  ...  0.83932567 -1.1606715\n",
            "   -0.44662362]\n",
            "  [ 0.09598656 -1.8996558   1.0799986  ...  0.7948678  -1.1515318\n",
            "   -0.43622872]]], shape=(64, 5, 512), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}